{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82a58e9-b0af-4c72-b553-187b9f66a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the dataset\n",
    "file_path = r'D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\jobstreet_with_monthly_min_max.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57873097-ac84-48d2-9a00-edb312820f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_MAPPING = {\n",
    "    'Kuala Lumpur': ['Alam Damai', 'Bandar Malaysia', 'Bandar Sri Permaisuri', 'Bandar Tasik Selatan', \n",
    "                     'Bandar Tun Razak', 'Bangsar', 'Bangsar Baru', 'Bangsar South', 'Brickfields', \n",
    "                     'Bukit Bintang', 'Bukit Damansara', 'Bukit Jalil', 'Bukit Kiara', 'Bukit Tunku', \n",
    "                     'Cheras', 'Chow Kit', 'Desa Pandan', 'Desa Parkcity', 'Desa Petaling', 'Jinjang', \n",
    "                     'Kepong', 'Keramat', 'Kuchai Lama', 'Lembah Pantai', 'Menjalara', 'Setapak', \n",
    "                     'Sri Damansara', 'Sri Hartamas', 'Sungai Besi', 'Taman Melawati', 'Taman Tun Dr Ismail', \n",
    "                     'Wangsa Maju'],\n",
    "\n",
    "    'Selangor': ['Ampang', 'Ara Damansara', 'Balakong', 'Bandar Amanjaya', 'Bandar Baru Bangi', \n",
    "                 'Bandar Baru Klang', 'Bandar Baru Selayang', 'Bandar Botanic', 'Bandar Bukit Puchong', \n",
    "                 'Bandar Bukit Tinggi', 'Bandar Kinrara', 'Bandar Mahkota Cheras', 'Bandar Rimbayu', \n",
    "                 'Bandar Saujana Putra', 'Bandar Sri Damansara', 'Bangi', 'Banting', 'Batang Kali', \n",
    "                 'Bestari Jaya', 'Bukit Beruntung', 'Bukit Jelutong', 'Bukit Raja', 'Bukit Rimau', \n",
    "                 'Cheras', 'Cyberjaya', 'Damansara', 'Damansara Damai', 'Damansara Jaya', \n",
    "                 'Damansara Perdana', 'Damansara Utama', 'Elmina', 'Gombak', 'Hulu Klang', \n",
    "                 'Hulu Langat', 'Kajang', 'Klang', 'Kinrara', 'Kota Damansara', 'Kota Warisan', \n",
    "                 'Puchong', 'Rawang', 'Sabak Bernam', 'Selayang', 'Semenyih', 'Seri Kembangan', \n",
    "                 'Shah Alam', 'Sungai Buloh', 'USJ'],\n",
    "\n",
    "    'Penang': ['Alma', 'Ayer Itam', 'Balik Pulau', 'Batu Ferringhi', 'Batu Kawan', 'Batu Maung', \n",
    "               'Bayan Baru', 'Bayan Lepas', 'Bukit Bendera', 'Bukit Mertajam', 'Bukit Minyak', \n",
    "               'Bukit Tengah', 'Butterworth', 'Farlim', 'George Town', 'Jelutong', 'Juru', \n",
    "               'Kepala Batas', 'Perai', 'Paya Terubong', 'Pulau Pinang', 'Relau', 'Sungai Ara', \n",
    "               'Simpang Ampat'],\n",
    "\n",
    "    'Melaka': ['Alor Gajah', 'Ayer Keroh', 'Bachang', 'Bandar Hilir', 'Batu Berendam', 'Bukit Rambai', \n",
    "               'Durian Tunggal', 'Jasin', 'Klebang', 'Melaka City', 'Melaka Tengah', 'Sungai Udang'],\n",
    "\n",
    "    'Johor': ['Johor Bahru', 'Pasir Gudang', 'Kulai', 'Ulu Tiram', 'Pontian', 'Kota Tinggi', 'Segamat', \n",
    "              'Batu Pahat', 'Muar', 'Mersing', 'Kluang'],\n",
    "\n",
    "    'Pahang': ['Bentong', 'Cameron Highlands', 'Chenor', 'Jerantut', 'Karak', 'Kuantan', 'Lipis', \n",
    "               'Mentakab', 'Pekan', 'Raub', 'Temerloh'],\n",
    "\n",
    "    'Perak': ['Ipoh', 'Batu Gajah', 'Kampar', 'Kuala Kangsar', 'Lumut', 'Parit Buntar', 'Simpang Pulai', \n",
    "              'Taiping', 'Teluk Intan'],\n",
    "\n",
    "    'Sarawak': ['Bintulu', 'Kapit', 'Kuching', 'Miri', 'Sibu', 'Sri Aman'],\n",
    "\n",
    "    'Sabah': ['Beaufort', 'Kota Kinabalu', 'Kudat', 'Lahad Datu', 'Penampang', 'Sandakan', 'Tawau'],\n",
    "\n",
    "    'Negeri Sembilan': ['Seremban', 'Nilai', 'Port Dickson', 'Bandar Sri Sendayan'],\n",
    "\n",
    "    'Kelantan': ['Kota Bharu', 'Bachok', 'Pasir Mas', 'Tanah Merah', 'Tumpat'],\n",
    "\n",
    "    'Terengganu': ['Kuala Terengganu', 'Dungun', 'Marang', 'Kemaman', 'Besut'],\n",
    "\n",
    "    'Kedah': ['Alor Setar', 'Sungai Petani', 'Kulim', 'Baling'],\n",
    "\n",
    "    'Perlis': ['Kangar', 'Arau'],\n",
    "\n",
    "    'Putrajaya': ['Putrajaya'],\n",
    "\n",
    "    'Labuan': ['Labuan']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba81fad-5224-4f80-9f92-50dd9c2010c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    # Business & Finance\n",
    "    'Accounting': 'Business & Finance',\n",
    "    'Banking & Financial Services': 'Business & Finance',\n",
    "    'Insurance & Superannuation': 'Business & Finance',\n",
    "    \n",
    "    # Technology\n",
    "    'Information & Communication Technology': 'Technology',\n",
    "    'Science & Technology': 'Technology',\n",
    "    \n",
    "    # Management & Professional Services\n",
    "    'CEO & General Management': 'Management',\n",
    "    'Consulting & Strategy': 'Professional Services',\n",
    "    'Legal': 'Professional Services',\n",
    "    'Human Resources & Recruitment': 'Professional Services',\n",
    "    \n",
    "    # Engineering & Construction\n",
    "    'Engineering': 'Engineering & Construction',\n",
    "    'Construction': 'Engineering & Construction',\n",
    "    'Design & Architecture': 'Engineering & Construction',\n",
    "    'Mining, Resources & Energy': 'Engineering & Construction',\n",
    "    \n",
    "    # Healthcare & Social Services\n",
    "    'Healthcare & Medical': 'Healthcare',\n",
    "    'Community Services & Development': 'Social Services',\n",
    "    'Government & Defence': 'Social Services',\n",
    "    \n",
    "    # Sales & Marketing\n",
    "    'Sales': 'Sales & Marketing',\n",
    "    'Marketing & Communications': 'Sales & Marketing',\n",
    "    'Advertising, Arts & Media': 'Sales & Marketing',\n",
    "    \n",
    "    # Operations & Logistics\n",
    "    'Manufacturing, Transport & Logistics': 'Operations',\n",
    "    'Trades & Services': 'Operations',\n",
    "    'Retail & Consumer Products': 'Operations',\n",
    "    \n",
    "    # Hospitality & Lifestyle\n",
    "    'Hospitality & Tourism': 'Hospitality',\n",
    "    'Sport & Recreation': 'Lifestyle',\n",
    "    \n",
    "    # Education & Administration\n",
    "    'Education & Training': 'Education',\n",
    "    'Administration & Office Support': 'Administration',\n",
    "    \n",
    "    # Miscellaneous\n",
    "    'Farming, Animals & Conservation': 'Agriculture & Environment',\n",
    "    'Real Estate & Property': 'Real Estate',\n",
    "    'Self Employment': 'Other',\n",
    "    'Call Centre & Customer Service': 'Customer Service'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdfab2c3-75ca-44d7-a3bb-4c228df9a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in d:\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in d:\\anaconda3\\lib\\site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c8178f-b211-4b5b-934e-3ee0feccf20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#find mean\n",
    "def clean_salary(salary_str):\n",
    "    \"\"\"Robust salary cleaning that handles all special cases\"\"\"\n",
    "    if pd.isna(salary_str) or not isinstance(salary_str, str):\n",
    "        return None\n",
    "    \n",
    "    # Common replacements first\n",
    "    salary_str = salary_str.lower()\n",
    "    salary_str = (salary_str.replace('per month', '')\n",
    "                  .replace('p.m.', '')\n",
    "                  .replace('per annum', '')\n",
    "                  .replace('monthly', '')\n",
    "                  .replace('â€“', '-')  # Replace en dash with normal hyphen\n",
    "                  .replace('–', '-'))  # Replace em dash with normal hyphen\n",
    "    \n",
    "    # Remove all non-numeric characters except digits, commas, and hyphens\n",
    "    salary_str = re.sub(r'[^\\d,\\-]', '', salary_str)\n",
    "    \n",
    "    # Handle cases where no numbers exist\n",
    "    if not any(char.isdigit() for char in salary_str):\n",
    "        return None\n",
    "    \n",
    "    # Extract all numbers (handle thousand separators)\n",
    "    numbers = []\n",
    "    for num in re.findall(r'[\\d,]+', salary_str):\n",
    "        try:\n",
    "            numbers.append(int(num.replace(',', '')))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Calculate mean if valid\n",
    "    if len(numbers) == 0:\n",
    "        return None\n",
    "    elif len(numbers) == 1:\n",
    "        return numbers[0]\n",
    "    else:\n",
    "        return round(sum(numbers) / len(numbers))\n",
    "\n",
    "# ================== STATE DETECTION ==================\n",
    "\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "\n",
    "def get_state(location):\n",
    "    try:\n",
    "        loc = geolocator.geocode(location, addressdetails=True)\n",
    "        return loc.raw['address'].get('state', 'Unknown')\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "df['state'] = df['location'].apply(get_state)\n",
    "\n",
    "def detect_state(location):\n",
    "    \"\"\"Identify Malaysian state from location text\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return None\n",
    "    \n",
    "    location = str(location).lower()\n",
    "    \n",
    "    # 1. Check for state names directly\n",
    "    for state in STATE_MAPPING.keys():\n",
    "        if any(keyword in location for keyword in [state] + [' ' + s for s in STATE_MAPPING[state]]):\n",
    "            return state.title()\n",
    "    \n",
    "    # 2. Fuzzy match as fallback\n",
    "    all_keywords = [kw for kws in STATE_MAPPING.values() for kw in kws]\n",
    "    match, score = process.extractOne(location, all_keywords)\n",
    "    if score > 70:\n",
    "        for state, keywords in STATE_MAPPING.items():\n",
    "            if match in keywords:\n",
    "                return state.title()\n",
    "    return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5324ce-d289-4b1a-8b5f-55ec32582065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added 'broad_category' column and saved to:\n",
      "D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\preprocessed_dataset.csv\n",
      "\n",
      "New category distribution:\n",
      "broad_category\n",
      "Business & Finance            6371\n",
      "Sales & Marketing             5129\n",
      "Engineering & Construction    4116\n",
      "Operations                    3705\n",
      "Administration                3575\n",
      "Technology                    2807\n",
      "Professional Services         1923\n",
      "Customer Service              1099\n",
      "Hospitality                    552\n",
      "Healthcare                     531\n",
      "Education                      394\n",
      "Real Estate                    317\n",
      "Management                      58\n",
      "Agriculture & Environment       53\n",
      "Social Services                 32\n",
      "Lifestyle                       24\n",
      "Other                            1\n",
      "Name: count, dtype: int64\n",
      "✅ Processed data saved to: D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\preprocessed_dataset.csv\n",
      "Original rows: 30687 | Cleaned rows: 30687\n",
      "\n",
      "Sample preview:\n",
      "Error during processing: \"['broad_category'] not in index\"\n"
     ]
    }
   ],
   "source": [
    "# ================== DATA PROCESSING ==================\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(r'D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\jobstreet_with_monthly_min_max.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# Process columns with error handling\n",
    "try:\n",
    "    df['mean_salary'] = df['salary'].apply(clean_salary)\n",
    "    df['state'] = df['location'].apply(get_state)\n",
    "    \n",
    "    # Remove rows where both salary and state are invalid\n",
    "    df_clean = df.dropna(subset=['mean_salary', 'state'], how='all')\n",
    "\n",
    "    # 2. Add the simplified category column\n",
    "    df['broad_category'] = df['category'].map(category_mapping)\n",
    "    \n",
    "    # Handle any unmapped categories (if needed)\n",
    "    df['broad_category'] = df['broad_category'].fillna('Other')\n",
    "    \n",
    "    # 3. Save back to the same file (or a new one)\n",
    "    output_path = r'D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\preprocessed_dataset.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Added 'broad_category' column and saved to:\\n{output_path}\")\n",
    "    print(\"\\nNew category distribution:\")\n",
    "    print(df['broad_category'].value_counts())\n",
    "    \n",
    "    print(f\"✅ Processed data saved to: {output_path}\")\n",
    "    print(f\"Original rows: {len(df)} | Cleaned rows: {len(df_clean)}\")\n",
    "    print(\"\\nSample preview:\")\n",
    "    print(df_clean[['job_title', 'location', 'state', 'salary', 'mean_salary','broad_category']].head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5107e8-d326-4fb9-84fd-0106c012ca68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googlemaps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(numbers) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(numbers))\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# ================== STATE DETECTION ==================\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgooglemaps\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeocoders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Nominatim\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googlemaps'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "#find mean\n",
    "def clean_salary(salary_str):\n",
    "    \"\"\"Robust salary cleaning that handles all special cases\"\"\"\n",
    "    if pd.isna(salary_str) or not isinstance(salary_str, str):\n",
    "        return None\n",
    "    \n",
    "    # Common replacements first\n",
    "    salary_str = salary_str.lower()\n",
    "    salary_str = (salary_str.replace('per month', '')\n",
    "                  .replace('p.m.', '')\n",
    "                  .replace('per annum', '')\n",
    "                  .replace('monthly', '')\n",
    "                  .replace('â€“', '-')  # Replace en dash with normal hyphen\n",
    "                  .replace('–', '-'))  # Replace em dash with normal hyphen\n",
    "    \n",
    "    # Remove all non-numeric characters except digits, commas, and hyphens\n",
    "    salary_str = re.sub(r'[^\\d,\\-]', '', salary_str)\n",
    "    \n",
    "    # Handle cases where no numbers exist\n",
    "    if not any(char.isdigit() for char in salary_str):\n",
    "        return None\n",
    "    \n",
    "    # Extract all numbers (handle thousand separators)\n",
    "    numbers = []\n",
    "    for num in re.findall(r'[\\d,]+', salary_str):\n",
    "        try:\n",
    "            numbers.append(int(num.replace(',', '')))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Calculate mean if valid\n",
    "    if len(numbers) == 0:\n",
    "        return None\n",
    "    elif len(numbers) == 1:\n",
    "        return numbers[0]\n",
    "    else:\n",
    "        return round(sum(numbers) / len(numbers))\n",
    "\n",
    "# ================== STATE DETECTION ==================\n",
    "\n",
    "import googlemaps\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "from rapidfuzz import process\n",
    "\n",
    "# Initialize APIs\n",
    "geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "gmaps = googlemaps.Client(key=\"YOUR_GOOGLE_API_KEY\")  # Replace with your API key\n",
    "\n",
    "# Function for direct state mapping + fuzzy matching\n",
    "def detect_state(location):\n",
    "    \"\"\"Identify Malaysian state from location text\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return None\n",
    "    \n",
    "    location = str(location).lower()\n",
    "    \n",
    "    # 1. Direct lookup in STATE_MAPPING\n",
    "    for state, locations in STATE_MAPPING.items():\n",
    "        if any(loc in location for loc in [state] + locations):\n",
    "            return state.title()\n",
    "    \n",
    "    # 2. Fuzzy matching as fallback\n",
    "    all_keywords = [kw for kws in STATE_MAPPING.values() for kw in kws]\n",
    "    match, score = process.extractOne(location, all_keywords)\n",
    "    if score > 70:\n",
    "        for state, keywords in STATE_MAPPING.items():\n",
    "            if match in keywords:\n",
    "                return state.title()\n",
    "    \n",
    "    return \"Unknown\"  # If state isn't found\n",
    "\n",
    "# Function for API-based state detection (for unknowns)\n",
    "def get_state_from_google(location):\n",
    "    \"\"\"Fetch state using Google Maps API\"\"\"\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(location)\n",
    "        for component in geocode_result[0]['address_components']:\n",
    "            if 'administrative_area_level_1' in component['types']:\n",
    "                return component['long_name']\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Unified column mapping function\n",
    "def map_location_to_state(location):\n",
    "    state = detect_state(location)\n",
    "    if state == \"Unknown\":  # Use Google Maps API for unknowns\n",
    "        state = get_state_from_google(location)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5607e3-f4ff-4482-832f-a9338229a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== DATA PROCESSING ==================\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(r'D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\jobstreet_with_monthly_min_max.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# Process columns with error handling\n",
    "try:\n",
    "    df['mean_salary'] = df['salary'].apply(clean_salary)\n",
    "    # Apply function to dataframe\n",
    "    df['state'] = df['location'].apply(map_location_to_state)\n",
    "    \n",
    "    # Remove rows where both salary and state are invalid\n",
    "    df_clean = df.dropna(subset=['mean_salary', 'state'], how='all')\n",
    "\n",
    "    # 2. Add the simplified category column\n",
    "    df['broad_category'] = df['category'].map(category_mapping)\n",
    "    \n",
    "    # Handle any unmapped categories (if needed)\n",
    "    df['broad_category'] = df['broad_category'].fillna('Other')\n",
    "    \n",
    "    # 3. Save back to the same file (or a new one)\n",
    "    output_path = r'D:\\Utem\\Y3S2\\BITI2513 DS\\dataset\\preprocessed_dataset2.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Added 'broad_category' column and saved to:\\n{output_path}\")\n",
    "    print(\"\\nNew category distribution:\")\n",
    "    print(df['broad_category'].value_counts())\n",
    "    \n",
    "    print(f\"✅ Processed data saved to: {output_path}\")\n",
    "    print(f\"Original rows: {len(df)} | Cleaned rows: {len(df_clean)}\")\n",
    "    print(\"\\nSample preview:\")\n",
    "    print(df_clean[['job_title', 'location', 'state', 'salary', 'mean_salary','broad_category']].head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097c42a-ce39-4819-9d7e-eb84203ddc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
